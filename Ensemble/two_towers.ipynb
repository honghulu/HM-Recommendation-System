{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "567_two_towers.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp9Ia2Y1_hby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8355a1d2-9f2c-46fc-a9ee-1c069eea0a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_recommenders"
      ],
      "metadata": {
        "id": "gHS-PCsYC56T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "\n",
        "data_path = '/content/gdrive/MyDrive/567_final_proj/data'\n",
        "# customers_df = pd.read_csv(data_path + \"/customers.csv\", header=0, dtype=str)\n",
        "# articles_df = pd.read_csv(data_path + \"/articles.csv\", header=0, dtype=str)\n",
        "# transactions_df = pd.read_csv(data_path + \"/transactions_train.csv\", header=0, dtype=str)\n",
        "# 16 weeks: 2020-06-02\n",
        "date = '2020-05-06'\n",
        "# # 8 weeks: 2020-07-28\n",
        "# date = '2020-07-28'\n",
        "# transactions_df = transactions_df[transactions_df.t_dat > date]\n",
        "# one year: '2019-09-22'\n",
        "transactions_df = pd.read_csv(data_path + \"/transactions_cut.csv\", header=0, dtype=str)\n",
        "customers_df = pd.read_csv(data_path + '/customers_clustered.csv', header=0, dtype=str)\n",
        "transactions_df = transactions_df[transactions_df.t_dat >= date]\n",
        "articles_df = pd.read_csv(data_path + \"/articles.csv\", header=0, dtype=str)\n",
        "\n",
        "cluster0_popularity = ['0706016001', '0720125001', '0706016002', '0610776002', '0372860001', '0759871002', '0751471001', '0706016003', '0464297007', '0372860002', '0562245046', '0610776001']\n",
        "cluster1_popularity = ['0706016001', '0706016002', '0720125001', '0372860001', '0610776002', '0759871002', '0751471001', '0706016003', '0464297007', '0372860002', '0562245046', '0448509014']\n",
        "cluster2_popularity = ['0706016001', '0720125001', '0706016002', '0372860001', '0759871002', '0610776002', '0751471001', '0706016003', '0464297007', '0372860002', '0562245046', '0448509014']\n",
        "\n",
        "customer_need_predict = customers_df[customers_df[\"customer_id\"].isin(transactions_df[\"customer_id\"].unique())]\n",
        "print(len(customer_need_predict))\n",
        "customer_need_popularity = customers_df[~customers_df[\"customer_id\"].isin(transactions_df[\"customer_id\"].unique())]\n",
        "print(len(customer_need_popularity))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAJkfq7J1mi1",
        "outputId": "613c07a3-1395-45d6-d788-ac65a9253f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "460815\n",
            "911165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10000\n",
        "TOP_K = 12\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(transactions_df[['article_id', 'customer_id']])))\n",
        "article_dataset = tf.data.Dataset.from_tensor_slices(articles_df['article_id'])\n",
        "\n",
        "class TwoTowerModel(tfrs.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        num_unique_users = 1371980\n",
        "        num_unique_movies = 105542\n",
        "        embedding_dim = 32\n",
        "        eval_batch_size = 128\n",
        "        \n",
        "        self.user_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=customers_df['customer_id'].to_numpy()),\n",
        "            tf.keras.layers.Embedding(num_unique_users+1, embedding_dim)\n",
        "        ])\n",
        "        \n",
        "        # Same for movies.\n",
        "        self.movie_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=articles_df['article_id'].to_numpy()),\n",
        "            tf.keras.layers.Embedding(num_unique_movies+1, embedding_dim)\n",
        "        ])\n",
        "        \n",
        "        self.task = tfrs.tasks.Retrieval(\n",
        "            metrics=tfrs.metrics.FactorizedTopK(candidates=article_dataset.batch(eval_batch_size).map(self.movie_model))\n",
        "        )\n",
        "        \n",
        "    def compute_loss(self, features, training=False):\n",
        "        user_embeddings = self.user_model(features['customer_id'])\n",
        "        movie_embeddings = self.movie_model(features['article_id'])\n",
        "        return self.task(user_embeddings, movie_embeddings)\n",
        "\n",
        "model = TwoTowerModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "model.fit(train_dataset.batch(BATCH_SIZE), verbose=True, epochs=3)\n",
        "\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model, k=TOP_K)\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((article_dataset.batch(100), article_dataset.batch(100).map(model.movie_model)))\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nakqRTqf_zk3",
        "outputId": "700e7e46-cc1b-46c6-fe81-3496bfd0f0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "573/573 [==============================] - 3086s 5s/step - factorized_top_k/top_1_categorical_accuracy: 4.5424e-06 - factorized_top_k/top_5_categorical_accuracy: 0.0011 - factorized_top_k/top_10_categorical_accuracy: 0.0022 - factorized_top_k/top_50_categorical_accuracy: 0.0097 - factorized_top_k/top_100_categorical_accuracy: 0.0174 - loss: 91892.8863 - regularization_loss: 0.0000e+00 - total_loss: 91892.8863\n",
            "Epoch 2/3\n",
            "573/573 [==============================] - 3068s 5s/step - factorized_top_k/top_1_categorical_accuracy: 2.6381e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0668 - factorized_top_k/top_10_categorical_accuracy: 0.0952 - factorized_top_k/top_50_categorical_accuracy: 0.1858 - factorized_top_k/top_100_categorical_accuracy: 0.2404 - loss: 89089.2851 - regularization_loss: 0.0000e+00 - total_loss: 89089.2851\n",
            "Epoch 3/3\n",
            "573/573 [==============================] - 3060s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.0092 - factorized_top_k/top_5_categorical_accuracy: 0.1237 - factorized_top_k/top_10_categorical_accuracy: 0.1602 - factorized_top_k/top_50_categorical_accuracy: 0.2602 - factorized_top_k/top_100_categorical_accuracy: 0.3158 - loss: 82009.1455 - regularization_loss: 0.0000e+00 - total_loss: 82009.1455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7f2d41fb2790>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"start prediction\")\n",
        "customer_ids = []\n",
        "predictions = []\n",
        "counter = 0\n",
        "for j in range(len(customers_df['customer_id'])):\n",
        "  c_id = customers_df['customer_id'][j]\n",
        "  if c_id in customer_need_predict['customer_id'].unique():\n",
        "    _, titles = index(tf.constant([c_id]))\n",
        "    preds = titles[0, :TOP_K]\n",
        "    predictions.append([i.decode(\"utf-8\") for i in preds.numpy()])\n",
        "  else:\n",
        "    cluster_id =  customers_df['clusters'][j]\n",
        "    if cluster_id == '0':\n",
        "      preds = cluster0_popularity\n",
        "    elif cluster_id == '1':\n",
        "      preds = cluster1_popularity\n",
        "    else:\n",
        "      preds = cluster2_popularity\n",
        "    predictions.append(preds)\n",
        "  customer_ids.append(c_id)\n",
        "  if counter % 1000 == 0:\n",
        "    print(counter)\n",
        "    print(f\"Recommendations for user {c_id}: {preds}\")\n",
        "  counter+=1\n",
        "\n",
        "results_df = pd.DataFrame(list(zip(customer_ids, predictions)), columns=['customer_id', 'prediction'])\n",
        "results_df[\"prediction\"] = results_df[\"prediction\"].apply(lambda x: \" \".join(x))\n",
        "print(results_df.head())\n",
        "results_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "u33iMZM-wDRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}