{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install recommenders\n!pip install tf_slim","metadata":{"execution":{"iopub.status.busy":"2022-05-01T23:08:33.27656Z","iopub.execute_input":"2022-05-01T23:08:33.276965Z","iopub.status.idle":"2022-05-01T23:08:55.703962Z","shell.execute_reply.started":"2022-05-01T23:08:33.276913Z","shell.execute_reply":"2022-05-01T23:08:55.702817Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom recommenders.utils.timer import Timer\nfrom recommenders.models.ncf.ncf_singlenode import NCF\nfrom recommenders.models.ncf.dataset import Dataset as NCFDataset\n# from recommenders.datasets import movielens\nfrom recommenders.datasets.python_splitters import python_chrono_split\nfrom recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n                                                     recall_at_k, get_top_k_items)\nfrom recommenders.utils.constants import SEED as DEFAULT_SEED\n\npp = ['0909370001', '0924243001', '0918522001', '0865799006', '0751471001', '0448509014', '0762846027', '0918292001', '0923758001', '0924243002', '0915529003', '0850917001']\n# Initial parameters\nTOP_K = 12\nEPOCHS = 50\nBATCH_SIZE = 1024\nSEED = DEFAULT_SEED\n\n# transaction_df = pd.read_csv(\"transactions_train.csv\", header=0)\ntransactions_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\", header=0)\ncustomers_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\", header=0)\narticles_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\", header=0)\nrec_df = pd.read_csv(\"../input/my-inputs/rec_item.csv\", header=0)\n# print(\"size of transaction:\", len(transactions_df))\n# transactions_df = transactions_df[:10000]\n# data processing:\ncustomers_df['userID'] = range(len(customers_df))\narticles_df['itemID'] = range(len(articles_df))\ntransactions_df = transactions_df.merge(customers_df[['customer_id', 'userID']], on='customer_id')\ntransactions_df = transactions_df.merge(articles_df[['article_id', 'itemID']], on='article_id')\nrec_df = rec_df.merge(customers_df[['customer_id', 'userID']], on='customer_id')\n\n# test_results = compute_test_results(model, train, validation, RATING_METRICS, RANKING_METRICS)\nprint(\"start data processing\")\n\"\"\"\ndate = '2020-05-06'\ntrain = transactions_df[transactions_df.t_dat <= date]\ntest = transactions_df[transactions_df.t_dat > date]\n\n\"\"\"\ntransactions_df['timestamp'] = transactions_df['t_dat'].apply(lambda x:time.mktime(time.strptime(x,'%Y-%m-%d')))\ntrain, test = python_chrono_split(transactions_df, [0.75, 0.25])\ntrain = train.drop(['timestamp', 't_dat'], axis=1)\ntest = test.drop(['timestamp', 't_dat'], axis=1)\n# header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n# t_dat, customer_id, article_id, price, sales_channel_id\n\ntrain = train.groupby(['userID', 'itemID']).size().reset_index().rename(columns={0:'rating'})\ntest = test.groupby(['userID', 'itemID']).size().reset_index().rename(columns={0:'rating'})\n\ntest = test[test[\"userID\"].isin(train[\"userID\"].unique())]\ntest = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n# leave_one_out_test = test.groupby(\"userID\").last().reset_index()\n\n\ntrain_file = \"./train.csv\"\ntest_file = \"./test.csv\"\n# leave_one_out_test_file = \"./leave_one_out_test.csv\"\n\nprint(\"save data\")\ntrain.to_csv(train_file, index=False)\ntest.to_csv(test_file, index=False)\n# leave_one_out_test.to_csv(leave_one_out_test_file, index=False)\nprint(train.head())\n# data = NCFDataset(train, test, seed=DEFAULT_SEED)\n# data = NCFDataset(train = train, test = test, seed=SEED)\ndata = NCFDataset(train_file=train_file, test_file=test_file, seed=SEED, overwrite_test_file_full=True)\nmodel = NCF (\n    n_users=data.n_users,\n    n_items=data.n_items,\n    model_type=\"NeuMF\",\n    n_factors=4,\n    layer_sizes=[16,8,4],\n    n_epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    learning_rate=1e-3,\n    verbose=1,\n    seed=SEED\n)\nprint(\"start model training\")\n# fitting the model\nmodel.fit(data)\n\"\"\"print(\"eval:\")\nfor user in test['userID']:\n    print(\"user:\", user)\n    per_user_pred = []\n    for item in articles_df['itemID']:\n        print(\"item:\", item)\n        if item not in train[\"itemID\"].unique():\n            pred = 0\n        else:\n            pred = model.predict(user, item)\n        per_user_pred.append((user, item, pred))\n    per_user_pred = sorted(per_user_pred, key=lambda pred: pred[2], reverse=True)[:12]\n    for article in test[test.userID == user]['itemID']:\n        if article in per_user_pred:\n            correct+=1\n            \nprint(\"accuracy:\", float(correct)/len(test))\n\"\"\"\n# predict the data in the test set\n# predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)] for (_, row) in test.iterrows()]\nprint(\"start prediction\")\ncustomer_ids = []\npredictions = []\ncounter1 = 0\ncounter2 = 0\nfor user in rec_df['userID']:\n    print(\"userID:\", user)\n    per_user_pred = []\n    recs = rec_df[rec_df.userID == user]['prediction'].iloc[0].split()\n    if user not in train['userID'].unique():\n        counter1+=1\n        customer_ids.append(customers_df[customers_df.userID == user][\"customer_id\"].iloc[0])\n        predictions.append(recs[:12])\n        continue\n    counter2+=1\n    for article_id in recs:\n        item = articles_df[articles_df.article_id == int(article_id)]['itemID'].iloc[0]\n        if item not in train[\"itemID\"].unique():\n            pred = 0\n        else:\n            pred = model.predict(user, item)\n        customer_id = customers_df[customers_df.userID == user][\"customer_id\"].iloc[0]\n        per_user_pred.append((customer_id, article_id, pred))\n    per_user_pred = sorted(per_user_pred, key=lambda pred: pred[2], reverse=True)[:12]\n    customer_ids.append(customer_id)\n    predictions.append([tuple[1] for tuple in per_user_pred])\n\nprint(\"generating results\")\nresults_df = pd.DataFrame(list(zip(customer_ids, predictions)), columns=['customer_id', 'prediction'])\nresults_df[\"prediction\"] = results_df[\"prediction\"].apply(lambda x: \" \".join(x))\nprint(results_df.head())\nresults_df.to_csv('submission.csv', index=False)\nprint(\"the user counter that not inside\", counter1)\nprint(\"inside:\", counter2)\n\"\"\"\nwith Timer() as test_time:\n    for user in rec_df['userID']:\n        per_user_pred = []\n        recs = rec_df[rec_df.userID == user]['prediction'].iloc[0].split()\n        if user not in train['userID'].unique():\n            customer_ids.append(customers_df[customers_df.userID == user][\"customer_id\"].iloc[0])\n            predictions.append(recs[:12])\n            continue\n        for article_id in recs:\n            print(\"article_id\", article_id)\n            item = articles_df[articles_df.article_id == int(article_id)]['itemID'].iloc[0]\n            print(\"item:\", item)\n            if item not in train[\"itemID\"].unique():\n                print(\"NOT! inside training set\")\n                pred = 0\n            else:\n                print(\"inside training set\")\n                pred = model.predict(user, item)\n            customer_id = customers_df[customers_df.userID == user][\"customer_id\"].iloc[0]\n            per_user_pred.append((customer_id, article_id, pred))\n        per_user_pred = sorted(per_user_pred, key=lambda pred: pred[2], reverse=True)[:12]\n        customer_ids.append(customer_id)\n        predictions.append([tuple[1] for tuple in per_user_pred])\n        print(\"final prediction:\", predictions)\n        break\n\nprint(\"Took {} seconds for prediction.\".format(test_time.interval))\n\"\"\"\n\"\"\"# predict the data in the test set\npredictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n               for (_, row) in test.iterrows()]\n\npredictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\nprint(predictions.head())\npredictions.to_csv(\"prediction.csv\", index=False)\n\"\"\"\n\n\"\"\"\nwith Timer() as test_time:\n\n    users, items, preds = [], [], []\n    item = list(train.itemID.unique())\n    for user in train.userID.unique():\n        user = [user] * len(item) \n        users.extend(user)\n        items.extend(item)\n        preds.extend(list(model.predict(user, item, is_list=True)))\n\n    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n\n    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n\nprint(\"Took {} seconds for prediction.\".format(test_time.interval))\n\neval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n\nprint(\"MAP:\\t%f\" % eval_map,\n      \"NDCG:\\t%f\" % eval_ndcg,\n      \"Precision@K:\\t%f\" % eval_precision,\n      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')\n\"\"\"\n\"\"\"\nk = TOP_K\n\nndcgs = []\nhit_ratio = []\n\nfor b in data.test_loader():\n    user_input, item_input, labels = b\n    output = model.predict(user_input, item_input, is_list=True)\n\n    output = np.squeeze(output)\n    rank = sum(output >= output[0])\n    if rank <= k:\n        ndcgs.append(1 / np.log(rank + 1))\n        hit_ratio.append(1)\n    else:\n        ndcgs.append(0)\n        hit_ratio.append(0)\n\neval_ndcg = np.mean(ndcgs)\neval_hr = np.mean(hit_ratio)\n\nprint(\"HR:\\t%f\" % eval_hr)\nprint(\"NDCG:\\t%f\" % eval_ndcg)\n\"\"\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T23:45:04.428751Z","iopub.execute_input":"2022-05-01T23:45:04.429082Z","iopub.status.idle":"2022-05-02T02:16:45.574074Z","shell.execute_reply.started":"2022-05-01T23:45:04.42905Z","shell.execute_reply":"2022-05-02T02:16:45.572806Z"},"trusted":true},"execution_count":null,"outputs":[]}]}